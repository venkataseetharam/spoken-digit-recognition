{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwoTWIysaNmc"
   },
   "source": [
    "<pre><font size=6>Spoken Digit Recognition</font></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_qGuPcj-aNmh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction =0.7\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HDBcl_PUaNmp"
   },
   "outputs": [],
   "source": [
    "#read the all file names in the recordings folder given by us\n",
    "#(if you get entire path, it is very useful in future)\n",
    "#save those files names as list in \"all_files\"\n",
    "root=r\"C:\\Users\\acer\\Downloads\\speech-recognition-assignment\\recordings\"\n",
    "all_files=[os.path.join(root,i) for i in os.listdir(root)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fWP6vXBeaNm3"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_audio=pd.DataFrame()\n",
    "label=[int(i[0]) for i in os.listdir(root)]\n",
    "df_audio[\"path\"]=all_files\n",
    "df_audio[\"label\"]=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5ZpuaGuJaNm8",
    "outputId": "76da0fb5-2033-49ef-eba7-880c395cf87a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   path    2000 non-null   object\n",
      " 1   label   2000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#info\n",
    "df_audio.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PlfssCc3aNnL"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df_audio = shuffle(df_audio, random_state=33)#don't change the random state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ448aENaNnR"
   },
   "source": [
    "<pre><font size=4>Train and Validation split</font></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vSPy-Ln6aNnS"
   },
   "outputs": [],
   "source": [
    "#split the data into train and validation and save in X_train, X_test, y_train, y_test\n",
    "#use stratify sampling\n",
    "#use random state of 45\n",
    "#use test size of 30%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df=train_test_split(df_audio,test_size=0.3,stratify=df_audio[\"label\"],random_state=45)\n",
    "y_train=train_df[\"label\"]\n",
    "y_test=test_df[\"label\"]\n",
    "X_train=train_df.drop(columns=[\"label\"])\n",
    "X_test=test_df.drop(columns=[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGhh-39vaNnb"
   },
   "source": [
    "<pre><font size=4>Preprocessing</font>\n",
    "\n",
    "All files are in the \"WAV\" format. We will read those raw data files using the librosa</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i99JacQSaNnc"
   },
   "outputs": [],
   "source": [
    "sample_rate = 22050\n",
    "def load_wav(x, get_duration=True):\n",
    "    '''This return the array values of audio with sampling rate of 22050 and Duration'''\n",
    "    #loading the wav file with sampling rate of 22050\n",
    "    samples, sample_rate = librosa.load(x, sr=22050)\n",
    "    if get_duration:\n",
    "        duration = librosa.get_duration(samples, sample_rate)\n",
    "        return [samples, duration]\n",
    "    else:\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\acer\\\\Downloads\\\\speech-recognition-assignment\\\\recordings\\\\6_yweweler_49.wav'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Rx97f8GGaNnh"
   },
   "outputs": [],
   "source": [
    "#use load_wav function that was written above to get every wave. \n",
    "#save it in X_train_processed and X_test_processed\n",
    "# X_train_processed/X_test_processed should be dataframes with two columns(raw_data, duration) with same index of X_train/y_train\n",
    "seq_tr=[]\n",
    "dur_tr=[]\n",
    "seq_te=[]\n",
    "dur_te=[]\n",
    "X_train_processed=pd.DataFrame()\n",
    "X_test_processed=pd.DataFrame()\n",
    "for i in X_train[\"path\"].values:\n",
    "    seq_tr.append(np.array(load_wav(i)[0]))\n",
    "    dur_tr.append(load_wav(i)[1])\n",
    "for i in X_test[\"path\"].values:\n",
    "    seq_te.append(np.array(load_wav(i)[0]))\n",
    "    dur_te.append(load_wav(i)[1])\n",
    "    \n",
    "X_train_processed[\"raw_data\"]=seq_tr\n",
    "X_test_processed[\"raw_data\"]=seq_te\n",
    "X_train_processed[\"duration\"]=dur_tr\n",
    "X_test_processed[\"duration\"]=dur_te\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([601., 674., 107.,  15.,   2.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([0.16104308, 0.36453061, 0.56801814, 0.77150567, 0.9749932 ,\n",
       "        1.17848073, 1.38196825, 1.58545578, 1.78894331, 1.99243084,\n",
       "        2.19591837]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X_train_processed[\"duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([601., 674., 107.,  15.,   2.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([0.16104308, 0.36453061, 0.56801814, 0.77150567, 0.9749932 ,\n",
       "        1.17848073, 1.38196825, 1.58545578, 1.78894331, 1.99243084,\n",
       "        2.19591837]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X_train_processed[\"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th percentile is 0.16104308390022676\n",
      "10 th percentile is 0.25848072562358276\n",
      "20 th percentile is 0.2974331065759637\n",
      "30 th percentile is 0.3297777777777778\n",
      "40 th percentile is 0.3569160997732426\n",
      "50 th percentile is 0.38945578231292516\n",
      "60 th percentile is 0.41328798185941046\n",
      "70 th percentile is 0.4444671201814059\n",
      "80 th percentile is 0.4826848072562358\n",
      "90 th percentile is 0.5531655328798186\n",
      "100 th percentile is 2.195918367346939\n"
     ]
    }
   ],
   "source": [
    "#print 0 to 100 percentile values with step size of 10 for train data duration. \n",
    "\n",
    "percentile = np.percentile(X_train_processed[\"duration\"],[i for i in range(0,101,10)])\n",
    "for i in range(len(percentile)):\n",
    "    print(i*10,\"th percentile is\",percentile[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rSlVQh4CaNn2",
    "outputId": "d6970436-db83-4d01-c910-7ebe92baab41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 th percentile is 0.5531655328798186\n",
      "91 th percentile is 0.5669832199546486\n",
      "92 th percentile is 0.5790349206349206\n",
      "93 th percentile is 0.599510657596372\n",
      "94 th percentile is 0.611208163265306\n",
      "95 th percentile is 0.6316009070294784\n",
      "96 th percentile is 0.6431455782312925\n",
      "97 th percentile is 0.6635741496598639\n",
      "98 th percentile is 0.6957514739229022\n",
      "99 th percentile is 0.79601179138322\n",
      "100 th percentile is 2.195918367346939\n"
     ]
    }
   ],
   "source": [
    "##print 90 to 100 percentile values with step size of 1. \n",
    "percentile = np.percentile(X_train_processed[\"duration\"],[i for i in range(90,101,1)])\n",
    "#print(percentile)\n",
    "for i  in range(90,101,1):\n",
    "    print(i,\"th percentile is\",percentile[i-90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cux3_jfcaNoM"
   },
   "source": [
    "<pre>Based on our analysis 99 percentile values are less than 0.8sec so we will limit maximum length of X_train_processed and X_test_processed to 0.8 sec. It is similar to pad_sequence for a text dataset. \n",
    "\n",
    "While loading the audio files, we are using sampling rate of 22050 so one sec will give array of length 22050. so, our maximum length is 0.8*22050 = 17640\n",
    "\n",
    "Pad with Zero if length of sequence is less than 17640 else Truncate the number. \n",
    "\n",
    "Also create a masking vector for train and test. \n",
    "\n",
    "masking vector value = 1 if it is real value, 0 if it is pad value. Masking vector data type must be bool.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "voqSEyvcaNoO"
   },
   "outputs": [],
   "source": [
    "max_length  = 17640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length  = 17640\n",
    "X_train_pad_seq=[]\n",
    "X_test_pad_seq=[]\n",
    "X_train_mask=[]\n",
    "X_test_mask=[]\n",
    "\n",
    "#Train data processing\n",
    "for i in X_train_processed[\"raw_data\"].values:\n",
    "    if len(i)>17640:\n",
    "        X_train_pad_seq.append(i[0:17640])\n",
    "        X_train_mask.append(np.array([1]*max_length))\n",
    "    elif len(i)<17640:\n",
    "        k=len(i)\n",
    "        i=np.pad(i,(0,17640-k))\n",
    "        X_train_pad_seq.append(i)\n",
    "        X_train_mask.append(np.concatenate((np.array([1]*k),np.array([0]*(17640-k)))))\n",
    "        \n",
    " #Test Data processing       \n",
    "for i in X_test_processed[\"raw_data\"].values:\n",
    "    if len(i)>17640:\n",
    "        X_test_pad_seq.append(i[0:17640])\n",
    "        X_test_mask.append(np.array([1]*max_length))\n",
    "    elif len(i)<17640:\n",
    "        k=len(i)\n",
    "        i=np.pad(i,(0,17640-k))\n",
    "        X_test_pad_seq.append(i)\n",
    "        X_test_mask.append(np.concatenate((np.array([1]*k),np.array([0]*(17640-k)))))\n",
    "\n",
    "X_train_pad_seq=np.array(X_train_pad_seq)\n",
    "X_test_pad_seq=np.array(X_test_pad_seq)\n",
    "X_train_mask=np.array(X_train_mask)\n",
    "X_train_mask=X_train_mask.astype(bool)\n",
    "X_test_mask=np.array(X_test_mask)\n",
    "X_test_mask=X_test_mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "B1-_r20BaNoW"
   },
   "outputs": [],
   "source": [
    "## as discussed above, Pad with Zero if length of sequence is less than 17640 else Truncate the number. \n",
    "## save in the X_train_pad_seq, X_test_pad_seq\n",
    "## also Create masking vector X_train_mask, X_test_mask\n",
    "\n",
    "## all the X_train_pad_seq, X_test_pad_seq, X_train_mask, X_test_mask will be numpy arrays mask vector dtype must be bool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('bool')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mask.dtype    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0kaYQ1jaNop"
   },
   "source": [
    "### 1. Giving Raw data directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score Implementation Using Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1score implementations\n",
    "class Score(tf.keras.callbacks.Callback):        \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.score={\"train_f1score\":[],\"test_f1score\":[],\"val_acc\":[]}\n",
    "     \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        pred_tr=np.argmax(self.model.predict([X_train_pad_seq,X_train_mask]),axis=1)\n",
    "        pred_te=np.argmax(self.model.predict([X_test_pad_seq,X_test_mask]),axis=1)\n",
    "        f1_sc_tr=sklearn.metrics.f1_score(pred_tr,y_train,average=\"micro\")\n",
    "        f1_sc_te=sklearn.metrics.f1_score(pred_te,y_test,average=\"micro\")\n",
    "        print(\"Train f1-score is\",f1_sc_tr)\n",
    "        print(\"Test f1-score is\",f1_sc_te)\n",
    "        self.score[\"train_f1score\"].append(f1_sc_tr)\n",
    "        self.score[\"test_f1score\"].append(f1_sc_te)\n",
    "sc=Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "X8yg951AaNor"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (Raw Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input-layer (InputLayer)        [(None, 17640, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input-mask (InputLayer)         [(None, 17640)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 100)          40800       input-layer[0][0]                \n",
      "                                                                 input-mask[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           6464        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64)           256         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           3250        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           510         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 51,280\n",
      "Trainable params: 51,152\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(17640,1),name=\"input-layer\",dtype=float)\n",
    "mask_inp=Input(shape=(17640,),name=\"input-mask\",dtype=bool)\n",
    "lstm=LSTM(units=100,name=\"lstm\",activation=\"tanh\")\n",
    "model_lstm=lstm(inputs=inp,mask=mask_inp)\n",
    "dense_layer=Dense(64,activation=\"tanh\")(model_lstm)\n",
    "drop=Dropout(0.5)(dense_layer)\n",
    "bn=BatchNormalization()(drop)\n",
    "dense_layer=Dense(50,activation=\"tanh\")(bn)\n",
    "output=Dense(10,activation=\"softmax\")(dense_layer)\n",
    "model_one=Model(inputs=[inp,mask_inp],outputs=[output])\n",
    "model_one.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#Learning rate scheduler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "def dynamic_learning_rate(epoch,lr):\n",
    "    if (epoch+1)%2==0: \n",
    "        changed=lr*((1-0.10)**((epoch+1)//3)) # for every 3rd epoch decrasing learning rate by 10 percent\n",
    "        return changed\n",
    "    else:\n",
    "        return lr #if not multiple of 3rd epoch return lr itself\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=5,write_graph=True,write_grads=True)\n",
    "lr_scheduler=LearningRateScheduler(dynamic_learning_rate,verbose=1)\n",
    "callbacks=[tensorboard_callback,sc] #sc->f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "43/43 [==============================] - 39s 848ms/step - loss: 2.3304 - accuracy: 0.1042 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 35s 828ms/step - loss: 2.3401 - accuracy: 0.0906 - val_loss: 2.3026 - val_accuracy: 0.0900\n",
      "Train f1-score is 0.08928571428571429\n",
      "Test f1-score is 0.09\n",
      "Epoch 3/5\n",
      "43/43 [==============================] - 36s 836ms/step - loss: 2.3316 - accuracy: 0.1025 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 4/5\n",
      "43/43 [==============================] - 36s 843ms/step - loss: 2.3238 - accuracy: 0.1029 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 5/5\n",
      "43/43 [==============================] - 37s 856ms/step - loss: 2.3246 - accuracy: 0.0939 - val_loss: 2.3028 - val_accuracy: 0.1017\n",
      "Train f1-score is 0.10071428571428571\n",
      "Test f1-score is 0.10166666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20082bd80d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model-1\n",
    "model_one.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=[\"accuracy\"])\n",
    "model_one.fit(x=[X_train_pad_seq,X_train_mask],y=y_train, batch_size=32,epochs=5,verbose=1,steps_per_epoch=1400//32,validation_data=([X_test_pad_seq,X_test_mask],y_test),callbacks=[tensorboard_callback,sc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best validation f1-score for model-1 is obtained at epoch-5 ie 0.101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8086 (pid 12208), started 6:42:54 ago. (Use '!kill 12208' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-74b09b80e512b8e7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-74b09b80e512b8e7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8086;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/ --host localhost --port 8086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Fwk0X4zaNpR"
   },
   "source": [
    "### 2. Converting into spectrogram and giving spectrogram data as input  \n",
    "<pre>\n",
    "We can use librosa to convert raw data into spectrogram. A spectrogram shows the features in a two-dimensional representation with the\n",
    "intensity of a frequency at a point in time i.e we are converting Time domain to frequency domain. you can read more about this in https://pnsn.org/spectrograms/what-is-a-spectrogram\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nb5AGzTjaNpS"
   },
   "outputs": [],
   "source": [
    "def convert_to_spectrogram(raw_data):\n",
    "    '''converting to spectrogram'''\n",
    "    spectrum = librosa.feature.melspectrogram(y=raw_data, sr=sample_rate, n_mels=64)\n",
    "    logmel_spectrum = librosa.power_to_db(S=spectrum, ref=np.max)\n",
    "    return logmel_spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Raw Sequence to Spectogram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "B__rN4RjaNpc"
   },
   "outputs": [],
   "source": [
    "##use convert_to_spectrogram and convert every raw sequence in X_train_pad_seq and X_test_pad-seq.\n",
    "## save those all in the X_train_spectrogram and X_test_spectrogram ( These two arrays must be numpy arrays)\n",
    "##use convert_to_spectrogram and convert every raw sequence in X_train_pad_seq and X_test_pad-seq.\n",
    "## save those all in the X_train_spectrogram and X_test_spectrogram ( These two arrays must be numpy arrays)\n",
    "X_train_spectrogram=[]\n",
    "X_test_spectrogram=[]\n",
    "\n",
    "for i in X_train_pad_seq:\n",
    "    X_train_spectrogram.append(convert_to_spectrogram(i))\n",
    "\n",
    "for i in X_test_pad_seq:\n",
    "    X_test_spectrogram.append(convert_to_spectrogram(i))\n",
    "    \n",
    "X_train_spectrogram=np.array(X_train_spectrogram)\n",
    "X_test_spectrogram=np.array(X_test_spectrogram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr1ynYZnaNpj"
   },
   "source": [
    "<font size=4>Grader function 6 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "oniXBXcsaNpk",
    "outputId": "0f94ec35-98af-4b98-c501-35cbbfbd0a2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_spectrogram():\n",
    "    flag_shape = (X_train_spectrogram.shape==(1400,64, 35)) and (X_test_spectrogram.shape == (600, 64, 35))\n",
    "    return flag_shape\n",
    "grader_spectrogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxlEVyIYaNpt"
   },
   "source": [
    "<pre>\n",
    "Now we have\n",
    "\n",
    "Train data: X_train_spectrogram and y_train  \n",
    "Test data: X_test_spectrogram and y_test   \n",
    "\n",
    "We will create a LSTM model which takes this input. \n",
    "\n",
    "Task:\n",
    "\n",
    "1. Create an LSTM network which takes \"X_train_spectrogram\" as input and has to return output at every time step. \n",
    "2. Average the output of every time step and give this to the Dense layer of any size. \n",
    "(ex: Output from LSTM will be  (#., time_steps, features) average the output of every time step i.e, you should get (#.,time_steps) \n",
    "and then pass to dense layer )\n",
    "3. give the above output to Dense layer of size 10( output layer) and train the network with sparse categorical cross entropy.  \n",
    "4. Use tensorboard to plot the graphs of loss and metric(use micro F1 score as metric) and histograms of gradients. \n",
    "5. make sure that it won't overfit. \n",
    "6. You are free to include any regularization\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score Implementation for Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1score implementations\n",
    "class Score2(tf.keras.callbacks.Callback):        \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.score={\"train_f1score\":[],\"test_f1score\":[],\"val_acc\":[]}\n",
    "     \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        pred_tr=np.argmax(self.model.predict(X_train_spectrogram),axis=1)\n",
    "        pred_te=np.argmax(self.model.predict(X_test_spectrogram),axis=1)\n",
    "        f1_sc_tr=sklearn.metrics.f1_score(pred_tr,y_train,average=\"micro\")\n",
    "        f1_sc_te=sklearn.metrics.f1_score(pred_te,y_test,average=\"micro\")\n",
    "        print(\"Train f1-score is\",f1_sc_tr)\n",
    "        print(\"Test f1-score is\",f1_sc_te)\n",
    "        self.score[\"train_f1score\"].append(f1_sc_tr)\n",
    "        self.score[\"test_f1score\"].append(f1_sc_te)\n",
    "sc2=Score2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "862fP2e-aNp3"
   },
   "outputs": [],
   "source": [
    "inp=Input(shape=(64,35),name=\"input_layer\")\n",
    "lstm=LSTM(units=256,activation=\"tanh\",return_sequences=True)(inp)\n",
    "glob=tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(lstm)\n",
    "den=Dense(128,activation=\"relu\",name=\"dense-layer1\")(glob)\n",
    "drop=Dropout(0.5)(den)\n",
    "den=Dense(64,activation=\"relu\",name=\"dense-layer2\")(drop)\n",
    "bn=BatchNormalization()(den)\n",
    "den=Dense(32,activation=tf.keras.layers.LeakyReLU(),name=\"dense-layer3\")(bn)\n",
    "output=Dense(10,activation=\"softmax\",name=\"output-layer\")(den)\n",
    "model_two=Model(inputs=inp,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 64, 35)]          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64, 256)           299008    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense-layer1 (Dense)         (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense-layer2 (Dense)         (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense-layer3 (Dense)         (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 318,250\n",
      "Trainable params: 318,122\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_two.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 4/43 [=>............................] - ETA: 1s - loss: 2.3439 - accuracy: 0.0833WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0168s). Check your callbacks.\n",
      "43/43 [==============================] - 4s 44ms/step - loss: 2.2702 - accuracy: 0.1480 - val_loss: 2.2689 - val_accuracy: 0.2083\n",
      "Train f1-score is 0.21642857142857144\n",
      "Test f1-score is 0.20833333333333334\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.8086 - accuracy: 0.3224 - val_loss: 2.1816 - val_accuracy: 0.2767\n",
      "Train f1-score is 0.27714285714285714\n",
      "Test f1-score is 0.27666666666666667\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.3793 - accuracy: 0.4959 - val_loss: 2.0578 - val_accuracy: 0.3517\n",
      "Train f1-score is 0.35928571428571426\n",
      "Test f1-score is 0.3516666666666667\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.1344 - accuracy: 0.5815 - val_loss: 1.9379 - val_accuracy: 0.4983\n",
      "Train f1-score is 0.4742857142857143\n",
      "Test f1-score is 0.49833333333333335\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.9436 - accuracy: 0.6465 - val_loss: 1.7980 - val_accuracy: 0.5600\n",
      "Train f1-score is 0.555\n",
      "Test f1-score is 0.56\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.8341 - accuracy: 0.6876 - val_loss: 1.6696 - val_accuracy: 0.6067\n",
      "Train f1-score is 0.6135714285714285\n",
      "Test f1-score is 0.6066666666666667\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.7006 - accuracy: 0.7483 - val_loss: 1.4582 - val_accuracy: 0.7017\n",
      "Train f1-score is 0.7114285714285714\n",
      "Test f1-score is 0.7016666666666667\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.7225 - accuracy: 0.7295 - val_loss: 1.2280 - val_accuracy: 0.8017\n",
      "Train f1-score is 0.7964285714285714\n",
      "Test f1-score is 0.8016666666666666\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6161 - accuracy: 0.7556 - val_loss: 1.1130 - val_accuracy: 0.7933\n",
      "Train f1-score is 0.7835714285714286\n",
      "Test f1-score is 0.7933333333333333\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6619 - accuracy: 0.7533 - val_loss: 0.9589 - val_accuracy: 0.7850\n",
      "Train f1-score is 0.792142857142857\n",
      "Test f1-score is 0.785\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6301 - accuracy: 0.7467 - val_loss: 0.8003 - val_accuracy: 0.8283\n",
      "Train f1-score is 0.8364285714285714\n",
      "Test f1-score is 0.8283333333333334\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.5531 - accuracy: 0.7981 - val_loss: 0.7460 - val_accuracy: 0.7817\n",
      "Train f1-score is 0.7985714285714286\n",
      "Test f1-score is 0.7816666666666666\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.5960 - accuracy: 0.7734 - val_loss: 0.6075 - val_accuracy: 0.8183\n",
      "Train f1-score is 0.8114285714285714\n",
      "Test f1-score is 0.8183333333333332\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.5883 - accuracy: 0.7862 - val_loss: 0.4588 - val_accuracy: 0.8733\n",
      "Train f1-score is 0.8785714285714286\n",
      "Test f1-score is 0.8733333333333333\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5133 - accuracy: 0.8125 - val_loss: 0.4519 - val_accuracy: 0.8633\n",
      "Train f1-score is 0.875\n",
      "Test f1-score is 0.8633333333333333\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.5265 - accuracy: 0.7960 - val_loss: 0.3912 - val_accuracy: 0.8900\n",
      "Train f1-score is 0.9064285714285715\n",
      "Test f1-score is 0.89\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.5129 - accuracy: 0.8278 - val_loss: 0.4677 - val_accuracy: 0.8383\n",
      "Train f1-score is 0.8592857142857143\n",
      "Test f1-score is 0.8383333333333334\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.5321 - accuracy: 0.7995 - val_loss: 0.4030 - val_accuracy: 0.8667\n",
      "Train f1-score is 0.89\n",
      "Test f1-score is 0.8666666666666667\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.5611 - accuracy: 0.7878 - val_loss: 0.3544 - val_accuracy: 0.8700\n",
      "Train f1-score is 0.8957142857142857\n",
      "Test f1-score is 0.87\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.5400 - accuracy: 0.7847 - val_loss: 0.3884 - val_accuracy: 0.8783\n",
      "Train f1-score is 0.8878571428571429\n",
      "Test f1-score is 0.8783333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24ad16c90d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model\n",
    "model_two.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=[\"accuracy\"])\n",
    "model_two.fit(x=X_train_spectrogram,y=y_train,batch_size=32,epochs=20,verbose=1,steps_per_epoch=1400//32,validation_data=(X_test_spectrogram,y_test),callbacks=[tensorboard_callback,sc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Validation F1-score is at epoch 20 ie 0.87833333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Visualisation for Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8084 (pid 18644), started 4:38:41 ago. (Use '!kill 18644' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8a1545bbccb6afbe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8a1545bbccb6afbe\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8084;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/ --host localhost --port 8084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSl8ZOXjaNqJ"
   },
   "source": [
    "### 3. data augmentation  \n",
    "<pre>\n",
    "Till now we have done with 2000 samples only. It is very less data. We are giving the process of generating augmented data below.\n",
    "\n",
    "There are two types of augmentation:\n",
    "1. time stretching - Time stretching either increases or decreases the length of the file. For time stretching we move the file 30% faster or slower\n",
    "2. pitch shifting - pitch shifting moves the frequencies higher or lower. For pitch shifting we shift up or down one half-step.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jR4JSEDgaNqK"
   },
   "outputs": [],
   "source": [
    "## generating augmented data. \n",
    "def generate_augmented_data(file_path):\n",
    "    augmented_data = []\n",
    "    samples = load_wav(file_path,get_duration=False)\n",
    "    for time_value in [0.7, 1, 1.3]:\n",
    "        for pitch_value in [-1, 0, 1]:\n",
    "            time_stretch_data = librosa.effects.time_stretch(samples, rate=time_value)\n",
    "            final_data = librosa.effects.pitch_shift(time_stretch_data, sr=sample_rate, n_steps=pitch_value)\n",
    "            augmented_data.append(final_data)\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QRdefb-SaNqS"
   },
   "outputs": [],
   "source": [
    "temp_path = df_audio.iloc[0].path\n",
    "aug_temp = generate_augmented_data(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "kzdG3iS-aNqc",
    "outputId": "0f17e45e-63a0-4986-8f69-05e2dbd71bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aug_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZckytZsraNqk"
   },
   "source": [
    "As discussed above, for one data point, we will get 9 augmented data points.  \n",
    "\n",
    "Split data into train and test (80-20 split)\n",
    "\n",
    "We have 2000 data points(1600 train points, 400 test points) \n",
    "\n",
    "Do augmentation only on train data, after augmentation we will get 14400 train points. \n",
    "\n",
    "do the above steps i.e training with raw data and spectrogram data with augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "AdKXVRlpaNql"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df=train_test_split(df_audio,test_size=0.2,stratify=df_audio[\"label\"],random_state=30)\n",
    "y_train=train_df[\"label\"]\n",
    "y_test=test_df[\"label\"]\n",
    "X_train=train_df.drop(columns=[\"label\"])\n",
    "X_test=test_df.drop(columns=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading ALL Augumented Data To X_train_augumented list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augumented=[]\n",
    "for i in X_train[\"path\"].values:\n",
    "    aug_file=generate_augmented_data(i)\n",
    "    for j in range(9):\n",
    "        X_train_augumented.append(aug_file[j])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp/ipykernel_8328/2018927794.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_train_augumented=np.array(X_train_augumented)\n"
     ]
    }
   ],
   "source": [
    "X_train_augumented=np.array(X_train_augumented)\n",
    "print(X_train_augumented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration=[]\n",
    "for i in X_train_augumented:\n",
    "    duration.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.277e+03, 5.376e+03, 6.630e+02, 6.300e+01, 3.000e+00, 6.000e+00,\n",
       "        6.000e+00, 0.000e+00, 0.000e+00, 6.000e+00]),\n",
       " array([ 2435. ,  9382.2, 16329.4, 23276.6, 30223.8, 37171. , 44118.2,\n",
       "        51065.4, 58012.6, 64959.8, 71907. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 th percentile is 14518.1\n",
      "91 th percentile is 14790.0\n",
      "92 th percentile is 15089.96\n",
      "93 th percentile is 15381.120000000024\n",
      "94 th percentile is 15971.299999999997\n",
      "95 th percentile is 16519.199999999997\n",
      "96 th percentile is 17416.159999999996\n",
      "97 th percentile is 18268.169999999955\n",
      "98 th percentile is 19539.0\n",
      "99 th percentile is 20958.400000000052\n",
      "100 th percentile is 71907.0\n"
     ]
    }
   ],
   "source": [
    "##print 90 to 100 percentile values with step size of 1. \n",
    "percentile = np.percentile(duration,[i for i in range(90,101,1)])\n",
    "for i  in range(90,101,1):\n",
    "    print(i,\"th percentile is\",percentile[i-90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting 98th percentile for max length and padding other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since taking 98th percentile ie 1954\n",
    "max_length  = 19540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed=pd.DataFrame()\n",
    "X_train_processed[\"raw_data\"]=X_train_augumented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use load_wav function that was written above to get every wave. \n",
    "#save it in X_train_processed and X_test_processed\n",
    "# X_train_processed/X_test_processed should be dataframes with two columns(raw_data, duration) with same index of X_train/y_train\n",
    "seq_te=[]\n",
    "X_test_processed=pd.DataFrame()\n",
    "for i in X_test[\"path\"].values:\n",
    "    seq_te.append(np.array(load_wav(i)[0]))\n",
    "    dur_te.append(load_wav(i)[1])\n",
    "X_test_processed[\"raw_data\"]=seq_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad_seq=[]\n",
    "X_test_pad_seq=[]\n",
    "X_train_mask=[]\n",
    "X_test_mask=[]\n",
    "\n",
    "#Train data processing\n",
    "for i in X_train_processed[\"raw_data\"].values:\n",
    "    if len(i)>max_length:\n",
    "        X_train_pad_seq.append(i[0:max_length])\n",
    "        X_train_mask.append(np.array([1]*max_length))\n",
    "    elif len(i)<max_length:\n",
    "        k=len(i)\n",
    "        i=np.pad(i,(0,max_length-k))\n",
    "        X_train_pad_seq.append(i)\n",
    "        X_train_mask.append(np.concatenate((np.array([1]*k),np.array([0]*(max_length-k)))))\n",
    "        \n",
    " #Test Data processing       \n",
    "for i in X_test_processed[\"raw_data\"].values:\n",
    "    if len(i)>max_length:\n",
    "        X_test_pad_seq.append(i[0:max_length])\n",
    "        X_test_mask.append(np.array([1]*max_length))\n",
    "    elif len(i)<max_length:\n",
    "        k=len(i)\n",
    "        i=np.pad(i,(0,max_length-k))\n",
    "        X_test_pad_seq.append(i)\n",
    "        X_test_mask.append(np.concatenate((np.array([1]*k),np.array([0]*(max_length-k)))))\n",
    "\n",
    "X_train_pad_seq=np.array(X_train_pad_seq)\n",
    "X_test_pad_seq=np.array(X_test_pad_seq)\n",
    "X_train_mask=np.array(X_train_mask)\n",
    "X_train_mask=X_train_mask.astype(bool)\n",
    "X_test_mask=np.array(X_test_mask)\n",
    "X_test_mask=X_test_mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14400, 19540)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating Labels since data is augumented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array(y_train)\n",
    "y_train=np.repeat(y_train,9,axis=0)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Rawdata(Augumented data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input-layer (InputLayer)        [(None, 19540, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input-mask (InputLayer)         [(None, 19540)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 100)          40800       input-layer[0][0]                \n",
      "                                                                 input-mask[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           6464        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50)           3250        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           510         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 51,280\n",
      "Trainable params: 51,152\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model-3\n",
    "inp=Input(shape=(19540,1),name=\"input-layer\",dtype=float)\n",
    "mask_inp=Input(shape=(19540,),name=\"input-mask\",dtype=bool)\n",
    "lstm=LSTM(units=100,name=\"lstm\",activation=\"tanh\")\n",
    "model_lstm=lstm(inputs=inp,mask=mask_inp)\n",
    "dense_layer=Dense(64,activation=\"tanh\")(model_lstm)\n",
    "drop=Dropout(0.6)(dense_layer)\n",
    "bn=BatchNormalization()(drop)\n",
    "dense_layer=Dense(50,activation=\"tanh\")(bn)\n",
    "output=Dense(10,activation=\"softmax\")(dense_layer)\n",
    "model_three=Model(inputs=[inp,mask_inp],outputs=[output])\n",
    "model_three.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14400, 19540)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 35s 764ms/step - loss: 2.3320 - accuracy: 0.0798 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 32s 745ms/step - loss: 2.3340 - accuracy: 0.1079 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 32s 758ms/step - loss: 2.3261 - accuracy: 0.0996 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10006944444444445\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 34s 781ms/step - loss: 2.3054 - accuracy: 0.1189 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 32s 747ms/step - loss: 2.3144 - accuracy: 0.1032 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 31s 729ms/step - loss: 2.3177 - accuracy: 0.1112 - val_loss: 2.3077 - val_accuracy: 0.1100\n",
      "Train f1-score is 0.09979166666666667\n",
      "Test f1-score is 0.11\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 32s 748ms/step - loss: 2.3233 - accuracy: 0.0957 - val_loss: 2.3060 - val_accuracy: 0.1125\n",
      "Train f1-score is 0.10319444444444445\n",
      "Test f1-score is 0.1125\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 31s 724ms/step - loss: 2.3103 - accuracy: 0.1168 - val_loss: 2.3044 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.0998611111111111\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 32s 744ms/step - loss: 2.3162 - accuracy: 0.1086 - val_loss: 2.3098 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.09993055555555555\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 32s 740ms/step - loss: 2.3159 - accuracy: 0.0912 - val_loss: 2.3125 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.100625\n",
      "Test f1-score is 0.10000000000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22052bdc670>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model-1\n",
    "model_three.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=[\"accuracy\"])\n",
    "model_three.fit(x=[X_train_pad_seq,X_train_mask],y=y_train, batch_size=32,epochs=10,verbose=1,steps_per_epoch=1400//32,validation_data=([X_test_pad_seq,X_test_mask],y_test),callbacks=[tensorboard_callback,sc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best validation f1-score is at epoch 7 ie 0.1125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8084 (pid 18644), started 4:39:06 ago. (Use '!kill 18644' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9253a7ec02c6427\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9253a7ec02c6427\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8084;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/ --host localhost --port 8084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting raw augumented data to Spectogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_spectrogram=[]\n",
    "X_test_spectrogram=[]\n",
    "\n",
    "for i in X_train_pad_seq:\n",
    "    X_train_spectrogram.append(convert_to_spectrogram(i))\n",
    "\n",
    "for i in X_test_pad_seq:\n",
    "    X_test_spectrogram.append(convert_to_spectrogram(i))\n",
    "    \n",
    "X_train_spectrogram=np.array(X_train_spectrogram)\n",
    "X_test_spectrogram=np.array(X_test_spectrogram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 64, 39)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_spectrogram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(64,39),name=\"input_layer\")\n",
    "lstm=LSTM(units=256,activation=\"tanh\",return_sequences=True)(inp)\n",
    "glob=tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(lstm)\n",
    "den=Dense(128,activation=\"relu\",name=\"dense-layer1\")(glob)\n",
    "drop=Dropout(0.5)(den)\n",
    "den=Dense(64,activation=\"relu\",name=\"dense-layer2\")(drop)\n",
    "bn=BatchNormalization()(den)\n",
    "den=Dense(32,activation=tf.keras.layers.LeakyReLU(),name=\"dense-layer3\")(bn)\n",
    "output=Dense(10,activation=\"softmax\",name=\"output-layer\")(den)\n",
    "model_four=Model(inputs=inp,outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 3/43 [=>............................] - ETA: 5s - loss: 2.2776 - accuracy: 0.2205WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_end` time: 0.0411s). Check your callbacks.\n",
      "43/43 [==============================] - 2s 30ms/step - loss: 2.2817 - accuracy: 0.1617 - val_loss: 2.2948 - val_accuracy: 0.1000\n",
      "Train f1-score is 0.10000000000000002\n",
      "Test f1-score is 0.10000000000000002\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 2.0697 - accuracy: 0.2749 - val_loss: 2.2276 - val_accuracy: 0.3375\n",
      "Train f1-score is 0.33576388888888886\n",
      "Test f1-score is 0.3375\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.6688 - accuracy: 0.3872 - val_loss: 2.1560 - val_accuracy: 0.2225\n",
      "Train f1-score is 0.20715277777777777\n",
      "Test f1-score is 0.2225\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.4158 - accuracy: 0.4777 - val_loss: 2.0787 - val_accuracy: 0.1725\n",
      "Train f1-score is 0.15138888888888888\n",
      "Test f1-score is 0.1725\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.3454 - accuracy: 0.4965 - val_loss: 1.9925 - val_accuracy: 0.2575\n",
      "Train f1-score is 0.21513888888888888\n",
      "Test f1-score is 0.2575\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.2025 - accuracy: 0.5716 - val_loss: 1.8791 - val_accuracy: 0.3725\n",
      "Train f1-score is 0.34902777777777777\n",
      "Test f1-score is 0.3725\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.1490 - accuracy: 0.5773 - val_loss: 1.7949 - val_accuracy: 0.3150\n",
      "Train f1-score is 0.2939583333333333\n",
      "Test f1-score is 0.315\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.1034 - accuracy: 0.5876 - val_loss: 1.6640 - val_accuracy: 0.4150\n",
      "Train f1-score is 0.38861111111111113\n",
      "Test f1-score is 0.415\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.0687 - accuracy: 0.6439 - val_loss: 1.6564 - val_accuracy: 0.4650\n",
      "Train f1-score is 0.44743055555555555\n",
      "Test f1-score is 0.465\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.0344 - accuracy: 0.6116 - val_loss: 1.2582 - val_accuracy: 0.7000\n",
      "Train f1-score is 0.6523611111111111\n",
      "Test f1-score is 0.7\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.0306 - accuracy: 0.6496 - val_loss: 1.2340 - val_accuracy: 0.6675\n",
      "Train f1-score is 0.6432638888888889\n",
      "Test f1-score is 0.6675\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.9488 - accuracy: 0.6323 - val_loss: 1.1093 - val_accuracy: 0.7375\n",
      "Train f1-score is 0.7347916666666666\n",
      "Test f1-score is 0.7375\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.9716 - accuracy: 0.6416 - val_loss: 0.9741 - val_accuracy: 0.7000\n",
      "Train f1-score is 0.6800694444444444\n",
      "Test f1-score is 0.7\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.8710 - accuracy: 0.6780 - val_loss: 0.9275 - val_accuracy: 0.7500\n",
      "Train f1-score is 0.6992361111111111\n",
      "Test f1-score is 0.75\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.9344 - accuracy: 0.6574 - val_loss: 0.7817 - val_accuracy: 0.7350\n",
      "Train f1-score is 0.6967361111111111\n",
      "Test f1-score is 0.735\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.8821 - accuracy: 0.6712 - val_loss: 0.6175 - val_accuracy: 0.8200\n",
      "Train f1-score is 0.7624305555555556\n",
      "Test f1-score is 0.82\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.8592 - accuracy: 0.6797 - val_loss: 0.6728 - val_accuracy: 0.7700\n",
      "Train f1-score is 0.7535416666666666\n",
      "Test f1-score is 0.7699999999999999\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.8884 - accuracy: 0.6508 - val_loss: 0.5664 - val_accuracy: 0.8250\n",
      "Train f1-score is 0.7560416666666666\n",
      "Test f1-score is 0.825\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.8538 - accuracy: 0.6638 - val_loss: 0.6216 - val_accuracy: 0.8000\n",
      "Train f1-score is 0.7422916666666666\n",
      "Test f1-score is 0.8000000000000002\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.8154 - accuracy: 0.7046 - val_loss: 0.5864 - val_accuracy: 0.7950\n",
      "Train f1-score is 0.75375\n",
      "Test f1-score is 0.795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2225a47a4c0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model\n",
    "model_four.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=[\"accuracy\"])\n",
    "model_four.fit(x=X_train_spectrogram,y=y_train,batch_size=32,epochs=20,verbose=1,steps_per_epoch=1400//32,validation_data=(X_test_spectrogram,y_test),callbacks=[tensorboard_callback,sc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best validation f1-score is at epoch 18 ie 0.825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard graph for visusalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8084 (pid 18644), started 4:43:51 ago. (Use '!kill 18644' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fa15b8e4b9b2cb95\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fa15b8e4b9b2cb95\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8084;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/ --host localhost --port 8084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "K0kaYQ1jaNop"
   ],
   "name": "Speech detection Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
